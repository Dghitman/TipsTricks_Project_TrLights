{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Collecting py7zr\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/45/8b/79453b7fb9d03f615d41c24b29f0c687cc32b114f2205d90bb5fb5fa4362/py7zr-0.14.1-py3-none-any.whl (68kB)\n",
      "\u001b[K     |████████████████████████████████| 71kB 6.0MB/s \n",
      "\u001b[?25hCollecting bcj-cffi<0.6.0,>=0.5.0\n",
      "  Downloading https://files.pythonhosted.org/packages/f4/a7/1610527d72957416fa25606062cb4151e7f7e697b8224aa47079d3d14d56/bcj_cffi-0.5.0-cp37-cp37m-manylinux2014_x86_64.whl\n",
      "Collecting multivolumefile<0.3.0,>=0.2.0\n",
      "  Downloading https://files.pythonhosted.org/packages/02/2d/c7b951e8624edc8f44e544203cb45e5bad4b493665ecc7e442a6ff6cd943/multivolumefile-0.2.2-py3-none-any.whl\n",
      "Collecting texttable\n",
      "  Downloading https://files.pythonhosted.org/packages/06/f5/46201c428aebe0eecfa83df66bf3e6caa29659dbac5a56ddfd83cae0d4a4/texttable-1.6.3-py2.py3-none-any.whl\n",
      "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from py7zr) (3.7.0)\n",
      "Collecting pyzstd<0.15.0,>=0.14.1; platform_python_implementation == \"CPython\"\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e9/50/766ed314ff309b0fde72aac1560e8902ba5210bb626cb57d44ada53ea42d/pyzstd-0.14.3-cp37-cp37m-manylinux2014_x86_64.whl (2.2MB)\n",
      "\u001b[K     |████████████████████████████████| 2.2MB 33.2MB/s \n",
      "\u001b[?25hCollecting ppmd-cffi<0.4.0,>=0.3.1\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/20/54/780ac526a6c81783c48e5341418f7295dcabfcb28b236eb412e806053e4c/ppmd_cffi-0.3.3-cp37-cp37m-manylinux2014_x86_64.whl (122kB)\n",
      "\u001b[K     |████████████████████████████████| 122kB 57.1MB/s \n",
      "\u001b[?25hCollecting pycryptodome\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ad/16/9627ab0493894a11c68e46000dbcc82f578c8ff06bc2980dcd016aea9bd3/pycryptodome-3.10.1-cp35-abi3-manylinux2010_x86_64.whl (1.9MB)\n",
      "\u001b[K     |████████████████████████████████| 1.9MB 57.4MB/s \n",
      "\u001b[?25hRequirement already satisfied: cffi>=1.14.0 in /usr/local/lib/python3.7/dist-packages (from bcj-cffi<0.6.0,>=0.5.0->py7zr) (1.14.5)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->py7zr) (3.7.4.3)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->py7zr) (3.4.0)\n",
      "Requirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi>=1.14.0->bcj-cffi<0.6.0,>=0.5.0->py7zr) (2.20)\n",
      "Installing collected packages: bcj-cffi, multivolumefile, texttable, pyzstd, ppmd-cffi, pycryptodome, py7zr\n",
      "Successfully installed bcj-cffi-0.5.0 multivolumefile-0.2.2 ppmd-cffi-0.3.3 py7zr-0.14.1 pycryptodome-3.10.1 pyzstd-0.14.3 texttable-1.6.3\n"
     ]
    }
   ],
   "source": [
    "!pip install py7zr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Uninstalling albumentations-0.1.12:\n",
      "  Successfully uninstalled albumentations-0.1.12\n",
      "Collecting albumentations\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/03/58/63fb1d742dc42d9ba2800ea741de1f2bc6bb05548d8724aa84794042eaf2/albumentations-0.5.2-py3-none-any.whl (72kB)\n",
      "\u001b[K     |████████████████████████████████| 81kB 6.3MB/s \n",
      "\u001b[?25hCollecting opencv-python-headless>=4.1.1\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/6d/6d/92f377bece9b0ec9c893081dbe073a65b38d7ac12ef572b8f70554d08760/opencv_python_headless-4.5.1.48-cp37-cp37m-manylinux2014_x86_64.whl (37.6MB)\n",
      "\u001b[K     |████████████████████████████████| 37.6MB 89kB/s \n",
      "\u001b[?25hCollecting imgaug>=0.4.0\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/66/b1/af3142c4a85cba6da9f4ebb5ff4e21e2616309552caca5e8acefe9840622/imgaug-0.4.0-py2.py3-none-any.whl (948kB)\n",
      "\u001b[K     |████████████████████████████████| 952kB 54.0MB/s \n",
      "\u001b[?25hRequirement already satisfied: scikit-image>=0.16.1 in /usr/local/lib/python3.7/dist-packages (from albumentations) (0.16.2)\n",
      "Requirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from albumentations) (3.13)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from albumentations) (1.4.1)\n",
      "Requirement already satisfied: numpy>=1.11.1 in /usr/local/lib/python3.7/dist-packages (from albumentations) (1.19.5)\n",
      "Requirement already satisfied: Pillow in /usr/local/lib/python3.7/dist-packages (from imgaug>=0.4.0->albumentations) (7.0.0)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from imgaug>=0.4.0->albumentations) (1.15.0)\n",
      "Requirement already satisfied: Shapely in /usr/local/lib/python3.7/dist-packages (from imgaug>=0.4.0->albumentations) (1.7.1)\n",
      "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from imgaug>=0.4.0->albumentations) (3.2.2)\n",
      "Requirement already satisfied: opencv-python in /usr/local/lib/python3.7/dist-packages (from imgaug>=0.4.0->albumentations) (4.1.2.30)\n",
      "Requirement already satisfied: imageio in /usr/local/lib/python3.7/dist-packages (from imgaug>=0.4.0->albumentations) (2.4.1)\n",
      "Requirement already satisfied: PyWavelets>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.16.1->albumentations) (1.1.1)\n",
      "Requirement already satisfied: networkx>=2.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.16.1->albumentations) (2.5)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->imgaug>=0.4.0->albumentations) (2.4.7)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->imgaug>=0.4.0->albumentations) (2.8.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->imgaug>=0.4.0->albumentations) (0.10.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->imgaug>=0.4.0->albumentations) (1.3.1)\n",
      "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.7/dist-packages (from networkx>=2.0->scikit-image>=0.16.1->albumentations) (4.4.2)\n",
      "Installing collected packages: opencv-python-headless, imgaug, albumentations\n",
      "  Found existing installation: imgaug 0.2.9\n",
      "    Uninstalling imgaug-0.2.9:\n",
      "      Successfully uninstalled imgaug-0.2.9\n",
      "Successfully installed albumentations-0.5.2 imgaug-0.4.0 opencv-python-headless-4.5.1.48\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.colab-display-data+json": {
       "pip_warning": {
        "packages": [
         "albumentations",
         "cv2",
         "imgaug"
        ]
       }
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Collecting git+https://github.com/albu/albumentations\n",
      "  Cloning https://github.com/albu/albumentations to /tmp/pip-req-build-14eyzsi_\n",
      "  Running command git clone -q https://github.com/albu/albumentations /tmp/pip-req-build-14eyzsi_\n",
      "Requirement already satisfied, skipping upgrade: numpy>=1.11.1 in /usr/local/lib/python3.7/dist-packages (from albumentations==0.5.2) (1.19.5)\n",
      "Requirement already satisfied, skipping upgrade: scipy in /usr/local/lib/python3.7/dist-packages (from albumentations==0.5.2) (1.4.1)\n",
      "Requirement already satisfied, skipping upgrade: scikit-image>=0.16.1 in /usr/local/lib/python3.7/dist-packages (from albumentations==0.5.2) (0.16.2)\n",
      "Requirement already satisfied, skipping upgrade: imgaug>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from albumentations==0.5.2) (0.4.0)\n",
      "Requirement already satisfied, skipping upgrade: PyYAML in /usr/local/lib/python3.7/dist-packages (from albumentations==0.5.2) (3.13)\n",
      "Requirement already satisfied, skipping upgrade: opencv-python>=4.1.1 in /usr/local/lib/python3.7/dist-packages (from albumentations==0.5.2) (4.1.2.30)\n",
      "Requirement already satisfied, skipping upgrade: PyWavelets>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.16.1->albumentations==0.5.2) (1.1.1)\n",
      "Requirement already satisfied, skipping upgrade: imageio>=2.3.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.16.1->albumentations==0.5.2) (2.4.1)\n",
      "Requirement already satisfied, skipping upgrade: pillow>=4.3.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.16.1->albumentations==0.5.2) (7.0.0)\n",
      "Requirement already satisfied, skipping upgrade: matplotlib!=3.0.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.16.1->albumentations==0.5.2) (3.2.2)\n",
      "Requirement already satisfied, skipping upgrade: networkx>=2.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.16.1->albumentations==0.5.2) (2.5)\n",
      "Requirement already satisfied, skipping upgrade: six in /usr/local/lib/python3.7/dist-packages (from imgaug>=0.4.0->albumentations==0.5.2) (1.15.0)\n",
      "Requirement already satisfied, skipping upgrade: Shapely in /usr/local/lib/python3.7/dist-packages (from imgaug>=0.4.0->albumentations==0.5.2) (1.7.1)\n",
      "Requirement already satisfied, skipping upgrade: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.16.1->albumentations==0.5.2) (0.10.0)\n",
      "Requirement already satisfied, skipping upgrade: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.16.1->albumentations==0.5.2) (2.4.7)\n",
      "Requirement already satisfied, skipping upgrade: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.16.1->albumentations==0.5.2) (1.3.1)\n",
      "Requirement already satisfied, skipping upgrade: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.16.1->albumentations==0.5.2) (2.8.1)\n",
      "Requirement already satisfied, skipping upgrade: decorator>=4.3.0 in /usr/local/lib/python3.7/dist-packages (from networkx>=2.0->scikit-image>=0.16.1->albumentations==0.5.2) (4.4.2)\n",
      "Building wheels for collected packages: albumentations\n",
      "  Building wheel for albumentations (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for albumentations: filename=albumentations-0.5.2-cp37-none-any.whl size=86173 sha256=1afca0bcb856556953391ef430b38f6bbeca827e710fc137117214575d738c1f\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-1nmb5waz/wheels/45/8b/e4/2837bbcf517d00732b8e394f8646f22b8723ac00993230188b\n",
      "Successfully built albumentations\n",
      "Installing collected packages: albumentations\n",
      "  Found existing installation: albumentations 0.5.2\n",
      "    Uninstalling albumentations-0.5.2:\n",
      "      Successfully uninstalled albumentations-0.5.2\n",
      "Successfully installed albumentations-0.5.2\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.colab-display-data+json": {
       "pip_warning": {
        "packages": [
         "albumentations"
        ]
       }
      }
     },
     "metadata": {}
    }
   ],
   "source": [
    "! pip uninstall albumentations --yes\n",
    "! pip install albumentations\n",
    "!pip install -U git+https://github.com/albu/albumentations --no-cache-dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import py7zr\n",
    "import sys\n",
    "import os\n",
    "from tqdm.notebook import tqdm\n",
    "import zipfile\n",
    "import yaml\n",
    "from lxml import objectify\n",
    "import pandas as pd\n",
    "import cv2\n",
    "from time import time\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, GroupKFold, StratifiedKFold\n",
    "import albumentations as A\n",
    "import albumentations.pytorch.transforms\n",
    "from albumentations.pytorch import ToTensor\n",
    "#from albumentations.pytorch.transforms import ToTensorV2\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from torchvision.models.detection import FasterRCNN\n",
    "from torchvision.models.detection.rpn import AnchorGenerator\n",
    "from torchvision.ops import nms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.utils.data.sampler import SequentialSampler\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ToTensorV2 as I couldn't import it https://github.com/albumentations-team/albumentations/blob/e3a2403f787fbb072ff45f165075b62236965709/albumentations/core/transforms_interface.py#L48\n",
    "\n",
    "import random\n",
    "from warnings import warn\n",
    "\n",
    "import cv2\n",
    "from copy import deepcopy\n",
    "\n",
    "from albumentations.core.serialization import SerializableMeta, get_shortest_class_fullname\n",
    "from albumentations.core.six import add_metaclass\n",
    "from albumentations.core.utils import format_args\n",
    "\n",
    "class BasicTransform:\n",
    "    call_backup = None\n",
    "\n",
    "    def __init__(self, always_apply=False, p=0.5):\n",
    "        self.p = p\n",
    "        self.always_apply = always_apply\n",
    "        self._additional_targets = {}\n",
    "\n",
    "        # replay mode params\n",
    "        self.deterministic = False\n",
    "        self.save_key = \"replay\"\n",
    "        self.params = {}\n",
    "        self.replay_mode = False\n",
    "        self.applied_in_replay = False\n",
    "\n",
    "    def __call__(self, *args, force_apply=False, **kwargs):\n",
    "        if args:\n",
    "            raise KeyError(\"You have to pass data to augmentations as named arguments, for example: aug(image=image)\")\n",
    "        if self.replay_mode:\n",
    "            if self.applied_in_replay:\n",
    "                return self.apply_with_params(self.params, **kwargs)\n",
    "\n",
    "            return kwargs\n",
    "\n",
    "        if (random.random() < self.p) or self.always_apply or force_apply:\n",
    "            params = self.get_params()\n",
    "\n",
    "            if self.targets_as_params:\n",
    "                assert all(key in kwargs for key in self.targets_as_params), \"{} requires {}\".format(\n",
    "                    self.__class__.__name__, self.targets_as_params\n",
    "                )\n",
    "                targets_as_params = {k: kwargs[k] for k in self.targets_as_params}\n",
    "                params_dependent_on_targets = self.get_params_dependent_on_targets(targets_as_params)\n",
    "                params.update(params_dependent_on_targets)\n",
    "            if self.deterministic:\n",
    "                if self.targets_as_params:\n",
    "                    warn(\n",
    "                        self.get_class_fullname() + \" could work incorrectly in ReplayMode for other input data\"\n",
    "                        \" because its' params depend on targets.\"\n",
    "                    )\n",
    "                kwargs[self.save_key][id(self)] = deepcopy(params)\n",
    "            return self.apply_with_params(params, **kwargs)\n",
    "\n",
    "        return kwargs\n",
    "\n",
    "    def apply_with_params(self, params, force_apply=False, **kwargs):  # skipcq: PYL-W0613\n",
    "        if params is None:\n",
    "            return kwargs\n",
    "        params = self.update_params(params, **kwargs)\n",
    "        res = {}\n",
    "        for key, arg in kwargs.items():\n",
    "            if arg is not None:\n",
    "                target_function = self._get_target_function(key)\n",
    "                target_dependencies = {k: kwargs[k] for k in self.target_dependence.get(key, [])}\n",
    "                res[key] = target_function(arg, **dict(params, **target_dependencies))\n",
    "            else:\n",
    "                res[key] = None\n",
    "        return res\n",
    "\n",
    "    def set_deterministic(self, flag, save_key=\"replay\"):\n",
    "        assert save_key != \"params\", \"params save_key is reserved\"\n",
    "        self.deterministic = flag\n",
    "        self.save_key = save_key\n",
    "        return self\n",
    "\n",
    "    def __repr__(self):\n",
    "        state = self.get_base_init_args()\n",
    "        state.update(self.get_transform_init_args())\n",
    "        return \"{name}({args})\".format(name=self.__class__.__name__, args=format_args(state))\n",
    "\n",
    "    def _get_target_function(self, key):\n",
    "        transform_key = key\n",
    "        if key in self._additional_targets:\n",
    "            transform_key = self._additional_targets.get(key, None)\n",
    "\n",
    "        target_function = self.targets.get(transform_key, lambda x, **p: x)\n",
    "        return target_function\n",
    "\n",
    "    def apply(self, img, **params):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def get_params(self):\n",
    "        return {}\n",
    "\n",
    "    @property\n",
    "    def targets(self):\n",
    "        # you must specify targets in subclass\n",
    "        # for example: ('image', 'mask')\n",
    "        #              ('image', 'boxes')\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def update_params(self, params, **kwargs):\n",
    "        if hasattr(self, \"interpolation\"):\n",
    "            params[\"interpolation\"] = self.interpolation\n",
    "        if hasattr(self, \"fill_value\"):\n",
    "            params[\"fill_value\"] = self.fill_value\n",
    "        if hasattr(self, \"mask_fill_value\"):\n",
    "            params[\"mask_fill_value\"] = self.mask_fill_value\n",
    "        params.update({\"cols\": kwargs[\"image\"].shape[1], \"rows\": kwargs[\"image\"].shape[0]})\n",
    "        return params\n",
    "\n",
    "    @property\n",
    "    def target_dependence(self):\n",
    "        return {}\n",
    "\n",
    "    def add_targets(self, additional_targets):\n",
    "        \"\"\"Add targets to transform them the same way as one of existing targets\n",
    "        ex: {'target_image': 'image'}\n",
    "        ex: {'obj1_mask': 'mask', 'obj2_mask': 'mask'}\n",
    "        by the way you must have at least one object with key 'image'\n",
    "        Args:\n",
    "            additional_targets (dict): keys - new target name, values - old target name. ex: {'image2': 'image'}\n",
    "        \"\"\"\n",
    "        self._additional_targets = additional_targets\n",
    "\n",
    "    @property\n",
    "    def targets_as_params(self):\n",
    "        return []\n",
    "\n",
    "    def get_params_dependent_on_targets(self, params):\n",
    "        raise NotImplementedError(\n",
    "            \"Method get_params_dependent_on_targets is not implemented in class \" + self.__class__.__name__\n",
    "        )\n",
    "\n",
    "    @classmethod\n",
    "    def get_class_fullname(cls):\n",
    "        return get_shortest_class_fullname(cls)\n",
    "\n",
    "    def get_transform_init_args_names(self):\n",
    "        raise NotImplementedError(\n",
    "            \"Class {name} is not serializable because the `get_transform_init_args_names` method is not \"\n",
    "            \"implemented\".format(name=self.get_class_fullname())\n",
    "        )\n",
    "\n",
    "    def get_base_init_args(self):\n",
    "        return {\"always_apply\": self.always_apply, \"p\": self.p}\n",
    "\n",
    "    def get_transform_init_args(self):\n",
    "        return {k: getattr(self, k) for k in self.get_transform_init_args_names()}\n",
    "\n",
    "    def _to_dict(self):\n",
    "        state = {\"__class_fullname__\": self.get_class_fullname()}\n",
    "        state.update(self.get_base_init_args())\n",
    "        state.update(self.get_transform_init_args())\n",
    "        return state\n",
    "\n",
    "    def get_dict_with_id(self):\n",
    "        d = self._to_dict()\n",
    "        d[\"id\"] = id(self)\n",
    "        return d\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class ToTensorV2(BasicTransform):\n",
    "    \"\"\"Convert image and mask to `torch.Tensor`.\n",
    "    Args:\n",
    "        transpose_mask (bool): if True and an input mask has three dimensions, this transform will transpose dimensions\n",
    "        so the shape `[height, width, num_channels]` becomes `[num_channels, height, width]`. The latter format is a\n",
    "        standard format for PyTorch Tensors. Default: False.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, transpose_mask=False, always_apply=True, p=1.0):\n",
    "        super(ToTensorV2, self).__init__(always_apply=always_apply, p=p)\n",
    "        self.transpose_mask = transpose_mask\n",
    "\n",
    "    @property\n",
    "    def targets(self):\n",
    "        return {\"image\": self.apply, \"mask\": self.apply_to_mask}\n",
    "\n",
    "    def apply(self, img, **params):  # skipcq: PYL-W0613\n",
    "        if len(img.shape) not in [2, 3]:\n",
    "            raise ValueError(\"Albumentations only supports images in HW or HWC format\")\n",
    "\n",
    "        if len(img.shape) == 2:\n",
    "            img = np.expand_dims(img, 2)\n",
    "\n",
    "        return torch.from_numpy(img.transpose(2, 0, 1))\n",
    "\n",
    "    def apply_to_mask(self, mask, **params):  # skipcq: PYL-W0613\n",
    "        if self.transpose_mask and mask.ndim == 3:\n",
    "            mask = mask.transpose(2, 0, 1)\n",
    "        return torch.from_numpy(mask)\n",
    "\n",
    "    def get_transform_init_args_names(self):\n",
    "        return (\"transpose_mask\",)\n",
    "\n",
    "    def get_params_dependent_on_targets(self, params):\n",
    "        return {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Download(name, url): #found this somewhere on stackoverflow\n",
    "  \"\"\"\n",
    "  Function for downloading datasets (except downloading from Google Drive)\n",
    "  \"\"\"\n",
    "  response = requests.get(url, stream=True)\n",
    "  total_size_in_bytes= int(response.headers.get('content-length', 0))\n",
    "  block_size = 1024 #1 Kibibyte\n",
    "  progress_bar = tqdm(total=total_size_in_bytes, unit='iB', unit_scale=True)\n",
    "  with open('/content/zipdata/'+name, 'wb') as file:\n",
    "      for data in response.iter_content(block_size):\n",
    "          progress_bar.update(len(data))\n",
    "          file.write(data)\n",
    "  progress_bar.close()\n",
    "  if total_size_in_bytes != 0 and progress_bar.n != total_size_in_bytes:\n",
    "      print(\"ERROR, something went wrong\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Directory \"/content/zipdata\" is created\nDirectory \"/content/unzipped\" is created\n"
     ]
    }
   ],
   "source": [
    "#Creating dirs to store datasets\n",
    "try:\n",
    "  os.makedirs('/content/zipdata')\n",
    "  print('Directory \"/content/zipdata\" is created')\n",
    "except:\n",
    "  print('Directory \"/content/zipdata\" already exists')\n",
    "try:\n",
    "  os.makedirs('/content/unzipped')\n",
    "  print('Directory \"/content/unzipped\" is created')\n",
    "except:\n",
    "  print('Directory \"/content/unzipped\" already exists')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "HBox(children=(FloatProgress(value=0.0, max=4520171347.0), HTML(value='')))",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "fae60b59e76a4d3d9a6144224698c28d"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "Download('lisa.zip','https://storage.googleapis.com/kaggle-data-sets/14302/19340/bundle/archive.zip?X-Goog-Algorithm=GOOG4-RSA-SHA256&X-Goog-Credential=gcp-kaggle-com%40kaggle-161607.iam.gserviceaccount.com%2F20210304%2Fauto%2Fstorage%2Fgoog4_request&X-Goog-Date=20210304T110637Z&X-Goog-Expires=259199&X-Goog-SignedHeaders=host&X-Goog-Signature=8277b8f99dca2998119b22a86849d8862d616cdc207395f1c04aab85170c49c5ba48319d154bdc219dfcbdc9a3c8cebec4acec558964033d76a9af41f46d0d68ea4bc407bb64072ddf747e30ab892a71b0c9f30ff293ad4eec0524b17644b7be320bfbd97fa960df056335ec4a63ea7aff5caf683f71ef69826c474c3a2a9045b6937d210a423c209add7a6f126c0ac8f1603440b7b0f696d928de510da797a89321ec07d44359103f0b7e4f24b61f0026c9f0be3564c34b81d7461c22ba543f7b29263d3118c5224c69d22f345325af2fc178ffa4d485dc8d04a32d0cd410eac0ea285b16bc14c36a7070ddcbf85f376ea2736081fddff850a9aeba3d2ba718')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with zipfile.ZipFile('/content/zipdata/lisa.zip', 'r') as zip_ref:\n",
    "    zip_ref.extractall('/content/unzipped/LISA')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "HBox(children=(FloatProgress(value=0.0, max=14361.0), HTML(value='')))",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "3597835d75af413a97059ed215502691"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "df_lisa = pd.DataFrame()\n",
    "for i in range(1,14):\n",
    "  df_temp = pd.read_csv('/content/unzipped/LISA/Annotations/Annotations/dayTrain/dayClip'+str(i)+'/frameAnnotationsBOX.csv', sep=';')\n",
    "  df_lisa = df_lisa.append(df_temp, ignore_index=True)\n",
    "\n",
    "df_lisa = df_lisa.drop(labels=['Origin file', 'Origin frame number','Origin track','Origin track frame number'], axis=1)\n",
    "\n",
    "df_lisa_left_names = df_lisa[df_lisa['Annotation tag'].isin(['stopLeft', 'goLeft', 'warningLeft'])]['Filename'].unique()\n",
    "df_lisa = df_lisa[~df_lisa['Filename'].isin(df_lisa_left_names)]\n",
    "\n",
    "df_lisa.reset_index(drop=True, inplace=True)\n",
    "\n",
    "df_lisa = df_lisa.rename(columns={'Filename':'Path', 'Annotation tag':'Color', 'Upper left corner X':'x1', \n",
    "                                  'Upper left corner Y':'y1', 'Lower right corner X':'x2', 'Lower right corner Y':'y2'})\n",
    "\n",
    "for i in tqdm(range(df_lisa.shape[0])): \n",
    "  df_lisa.loc[i,'Path'] = '/content/unzipped/LISA/dayTrain/dayTrain/dayClip'+df_lisa['Path'][i][19:-11]+'/frames/'+df_lisa['Path'][i][12:]\n",
    "\n",
    "df_lisa.replace('go', 'Green', inplace=True)\n",
    "df_lisa.replace('stop', 'Red', inplace=True)\n",
    "df_lisa.replace('warning', 'Yellow', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lisa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_lisa.sample(frac = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df.iloc[0:4000]\n",
    "all_df = df1.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_df = all_df.rename(columns={\"Path\": \"image_id\", \"Color\": \"label\", 'x1': 'x_min', 'y1':'y_min', 'x2':'x_max', 'y2':'y_max'})\n",
    "all_df['label'] = all_df['label'].replace(['Red'], 3)\n",
    "all_df['label'] = all_df['label'].replace(['Green'], 1)\n",
    "all_df['label'] = all_df['label'].replace(['Yellow'], 2)\n",
    "all_df = all_df.astype({\"x_min\": int, \"x_max\": int, 'y_min': int, 'y_max':int})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                                               image_id  label  ...  x_max  y_max\n",
       "0     /content/unzipped/LISA/dayTrain/dayTrain/dayCl...      3  ...    757    318\n",
       "1     /content/unzipped/LISA/dayTrain/dayTrain/dayCl...      3  ...    871    238\n",
       "2     /content/unzipped/LISA/dayTrain/dayTrain/dayCl...      1  ...   1181    338\n",
       "3     /content/unzipped/LISA/dayTrain/dayTrain/dayCl...      1  ...   1064    172\n",
       "4     /content/unzipped/LISA/dayTrain/dayTrain/dayCl...      3  ...   1014    260\n",
       "...                                                 ...    ...  ...    ...    ...\n",
       "3995  /content/unzipped/LISA/dayTrain/dayTrain/dayCl...      1  ...    968    316\n",
       "3996  /content/unzipped/LISA/dayTrain/dayTrain/dayCl...      1  ...    584    331\n",
       "3997  /content/unzipped/LISA/dayTrain/dayTrain/dayCl...      3  ...   1014    260\n",
       "3998  /content/unzipped/LISA/dayTrain/dayTrain/dayCl...      3  ...    626    356\n",
       "3999  /content/unzipped/LISA/dayTrain/dayTrain/dayCl...      1  ...   1181    338\n",
       "\n",
       "[4000 rows x 6 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>image_id</th>\n      <th>label</th>\n      <th>x_min</th>\n      <th>y_min</th>\n      <th>x_max</th>\n      <th>y_max</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>/content/unzipped/LISA/dayTrain/dayTrain/dayCl...</td>\n      <td>3</td>\n      <td>718</td>\n      <td>255</td>\n      <td>757</td>\n      <td>318</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>/content/unzipped/LISA/dayTrain/dayTrain/dayCl...</td>\n      <td>3</td>\n      <td>838</td>\n      <td>180</td>\n      <td>871</td>\n      <td>238</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>/content/unzipped/LISA/dayTrain/dayTrain/dayCl...</td>\n      <td>1</td>\n      <td>1157</td>\n      <td>293</td>\n      <td>1181</td>\n      <td>338</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>/content/unzipped/LISA/dayTrain/dayTrain/dayCl...</td>\n      <td>1</td>\n      <td>1022</td>\n      <td>92</td>\n      <td>1064</td>\n      <td>172</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>/content/unzipped/LISA/dayTrain/dayTrain/dayCl...</td>\n      <td>3</td>\n      <td>987</td>\n      <td>215</td>\n      <td>1014</td>\n      <td>260</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>3995</th>\n      <td>/content/unzipped/LISA/dayTrain/dayTrain/dayCl...</td>\n      <td>1</td>\n      <td>944</td>\n      <td>271</td>\n      <td>968</td>\n      <td>316</td>\n    </tr>\n    <tr>\n      <th>3996</th>\n      <td>/content/unzipped/LISA/dayTrain/dayTrain/dayCl...</td>\n      <td>1</td>\n      <td>572</td>\n      <td>311</td>\n      <td>584</td>\n      <td>331</td>\n    </tr>\n    <tr>\n      <th>3997</th>\n      <td>/content/unzipped/LISA/dayTrain/dayTrain/dayCl...</td>\n      <td>3</td>\n      <td>987</td>\n      <td>215</td>\n      <td>1014</td>\n      <td>260</td>\n    </tr>\n    <tr>\n      <th>3998</th>\n      <td>/content/unzipped/LISA/dayTrain/dayTrain/dayCl...</td>\n      <td>3</td>\n      <td>614</td>\n      <td>338</td>\n      <td>626</td>\n      <td>356</td>\n    </tr>\n    <tr>\n      <th>3999</th>\n      <td>/content/unzipped/LISA/dayTrain/dayTrain/dayCl...</td>\n      <td>1</td>\n      <td>1157</td>\n      <td>293</td>\n      <td>1181</td>\n      <td>338</td>\n    </tr>\n  </tbody>\n</table>\n<p>4000 rows × 6 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 17
    }
   ],
   "source": [
    "all_df"
   ]
  },
  {
   "source": [
    "## Model"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, test_df = train_test_split(all_df, test_size=0.25, random_state=42)\n",
    "train_df, val_df = train_test_split(train_df, test_size=0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 1\n",
    "BATCH_SIZE = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrafficLightsDataset:\n",
    "    def __init__(self, df, transforms=None):\n",
    "        super().__init__()\n",
    "\n",
    "        self.image_ids = df.image_id.unique()\n",
    "        self.df = df\n",
    "        self.transforms = transforms\n",
    "        \n",
    "    def __len__(self) -> int:\n",
    "        return self.image_ids.shape[0]\n",
    "\n",
    "    def __getitem__(self, index: int):\n",
    "\n",
    "        image_id = self.image_ids[index]\n",
    "        records = self.df[self.df.image_id == image_id]\n",
    "\n",
    "        image = cv2.imread(image_id)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB).astype(np.float32)\n",
    "        image /= 255.0\n",
    "        \n",
    "        boxes = records[['x_min','y_min','x_max','y_max']].values\n",
    "        boxes = torch.as_tensor(boxes,dtype=torch.float64)\n",
    "        #print(boxes)\n",
    "        \n",
    "        area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n",
    "        area = torch.as_tensor(area, dtype=torch.float32)\n",
    "        #print(area)\n",
    "\n",
    "        labels = torch.as_tensor(records.label.values, dtype=torch.int64)\n",
    "        \n",
    "        iscrowd = torch.zeros_like(labels, dtype=torch.int64)\n",
    "        \n",
    "        target = {}\n",
    "        target['boxes'] = boxes\n",
    "        target['labels'] = labels\n",
    "        target['image_id'] = torch.tensor([index])\n",
    "        target['area'] = area\n",
    "        target['iscrowd'] = iscrowd\n",
    "\n",
    "        if self.transforms:\n",
    "            sample = {\n",
    "                'image': image,\n",
    "                'bboxes': target['boxes'],\n",
    "                'labels': labels\n",
    "            }\n",
    "            sample = self.transforms(**sample)\n",
    "            image = sample['image']\n",
    "            \n",
    "            target['boxes'] = torch.as_tensor(sample['bboxes'],dtype=torch.float32)\n",
    "            target['labels'] = torch.as_tensor(sample['labels'])\n",
    "            \n",
    "        return image, target, image_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LossAverager:\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    return tuple(zip(*batch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTrainTransform():\n",
    "    return A.Compose([\n",
    "        A.Resize(height=256, width=256, p=1),\n",
    "        A.Flip(0.5),\n",
    "        ToTensorV2(p=1.0)\n",
    "    ], bbox_params={'format': 'pascal_voc', 'label_fields': ['labels']})\n",
    "\n",
    "def getValTransform():\n",
    "    return A.Compose([\n",
    "        A.Resize(height=256, width=256, p=1),\n",
    "        ToTensorV2(p=1.0)\n",
    "    ], bbox_params={'format': 'pascal_voc', 'label_fields': ['labels']})\n",
    "\n",
    "def getTestTransform():\n",
    "    return A.Compose([\n",
    "        A.Resize(height=256, width=256, p=1),\n",
    "        ToTensorV2(p=1.0)\n",
    "    ], bbox_params={'format': 'pascal_voc', 'label_fields': ['labels']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainDataset = TrafficLightsDataset(train_df,getTrainTransform())\n",
    "valDataset = TrafficLightsDataset(val_df,getValTransform())\n",
    "testDataset = TrafficLightsDataset(test_df,getTestTransform())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainDataLoader = DataLoader(\n",
    "    trainDataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=0,\n",
    "    collate_fn=collate_fn\n",
    ")\n",
    "\n",
    "valDataLoader = DataLoader(\n",
    "    valDataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=0,\n",
    "    collate_fn=collate_fn\n",
    ")\n",
    "\n",
    "testDataLoader = DataLoader(\n",
    "    testDataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=0,\n",
    "    collate_fn=collate_fn\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_instance_segmentation_model(num_classes):\n",
    "    model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "N_CLASS = 4 \n",
    "INP_FEATURES = model.roi_heads.box_predictor.cls_score.in_features\n",
    "model.roi_heads.box_predictor = FastRCNNPredictor(INP_FEATURES, N_CLASS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.to(device)\n",
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    "optimizer = torch.optim.Adam(params)\n",
    "lr_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "HBox(children=(FloatProgress(value=0.0, max=944.0), HTML(value='')))",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "2e9e3990818847809e474241f3734c5a"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n[06:07]\nEpoch 0/1\nTrain loss: 5727.683992561656\n"
     ]
    }
   ],
   "source": [
    "lossHist = LossAverager()\n",
    "valLossHist = LossAverager()\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    \n",
    "    start_time = time()\n",
    "    model.train()\n",
    "    lossHist.reset()\n",
    "    \n",
    "    for images, targets, image_ids in tqdm(trainDataLoader):\n",
    "        #bbox = check_bbox(bbox)\n",
    "        images = torch.stack(images).to(device)\n",
    "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "        \n",
    "        bs = images.shape[0]\n",
    "        \n",
    "        loss_dict = model(images, targets)\n",
    "        \n",
    "        totalLoss = sum(loss for loss in loss_dict.values())\n",
    "        lossValue = totalLoss.item()\n",
    "        \n",
    "        lossHist.update(lossValue,bs)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        totalLoss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    if lr_scheduler is not None:\n",
    "        lr_scheduler.step(totalLoss)\n",
    "\n",
    "    print(f\"[{str(datetime.timedelta(seconds = time() - start_time))[2:7]}]\")\n",
    "    print(f\"Epoch {epoch}/{EPOCHS}\")\n",
    "    print(f\"Train loss: {lossHist.avg}\")\n",
    "    \n",
    "    if(epoch == 10):\n",
    "        torch.save(model.state_dict(), 'fasterrcnn_resnet{}_fpn.pth'.format(epoch))"
   ]
  },
  {
   "source": [
    "## Saving model"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'fasterrcnn_resnet{}_fpn.pth'.format(epoch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "metadata": {},
     "execution_count": 66
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('/content/fasterrcnn_resnet0_fpn.pth'))"
   ]
  },
  {
   "source": [
    "## Testing on a video"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "HBox(children=(FloatProgress(value=0.0, max=78362042.0), HTML(value='')))",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d8984ad38fc345df96935153a782436a"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "Download('project.avi.zip','https://storage.googleapis.com/kaggle-data-sets/1021727/1722284/compressed/project.avi.zip?X-Goog-Algorithm=GOOG4-RSA-SHA256&X-Goog-Credential=gcp-kaggle-com%40kaggle-161607.iam.gserviceaccount.com%2F20210304%2Fauto%2Fstorage%2Fgoog4_request&X-Goog-Date=20210304T112452Z&X-Goog-Expires=259199&X-Goog-SignedHeaders=host&X-Goog-Signature=778ab8a44682a59633da1bb56f7ca4e92c41eef4f877d4a098ff118f8e0519f766e15d4e2dd876e60f192e60894fa61a24248d3bde0d7404a1973b125150756290fec8b1b419d780d809e283759784da1fc84bc8f47ad3b7eabb55a88950b663bb302b4fa512b73d80615a6f2a9e79bea1847a8e822e6a81d9dee80f62f21fec6d75fb78784f9d45b41fccea0e41f1094734b7ab3d97bce56506938e7881e802319c077eb49b895c293cc135a8796dfbaf121eb4bf8b2a90b18bb2b520596d137bedaa7e3ac4b9c364083e60d40f6e0edf968bb3e7bcb22de33a14ae00550fe76a7d86eb707a938b4bb51999987fd3e6c0be9aa9f00a77cff739b082ded31f8c')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "with zipfile.ZipFile('/content/zipdata/project.avi.zip', 'r') as zip_ref:\n",
    "    zip_ref.extractall('/content/unzipped/project.avi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess = transforms.Compose([\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "cap = cv2.VideoCapture('/content/unzipped/project.avi/project.avi')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<VideoCapture 0x7f20c666dcb0>"
      ]
     },
     "metadata": {},
     "execution_count": 78
    }
   ],
   "source": [
    "cap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([[[[0.9804, 0.9804, 0.9804,  ..., 0.9765, 0.9765, 0.9765],\n          [0.9804, 0.9804, 0.9804,  ..., 0.9765, 0.9765, 0.9765],\n          [0.9804, 0.9804, 0.9804,  ..., 0.9765, 0.9765, 0.9765],\n          ...,\n          [0.1412, 0.1412, 0.1412,  ..., 0.1176, 0.1176, 0.1176],\n          [0.1412, 0.1412, 0.1412,  ..., 0.1176, 0.1176, 0.1176],\n          [0.1412, 0.1412, 0.1412,  ..., 0.1176, 0.1176, 0.1176]],\n\n         [[0.9922, 0.9922, 0.9922,  ..., 0.8196, 0.8196, 0.8196],\n          [0.9922, 0.9922, 0.9922,  ..., 0.8196, 0.8196, 0.8196],\n          [0.9922, 0.9922, 0.9922,  ..., 0.8196, 0.8196, 0.8196],\n          ...,\n          [0.0824, 0.0824, 0.0824,  ..., 0.0784, 0.0784, 0.0784],\n          [0.0824, 0.0824, 0.0824,  ..., 0.0784, 0.0784, 0.0784],\n          [0.0824, 0.0824, 0.0824,  ..., 0.0784, 0.0784, 0.0784]],\n\n         [[0.9843, 0.9843, 0.9843,  ..., 0.6235, 0.6235, 0.6235],\n          [0.9843, 0.9843, 0.9843,  ..., 0.6235, 0.6235, 0.6235],\n          [0.9843, 0.9843, 0.9843,  ..., 0.6235, 0.6235, 0.6235],\n          ...,\n          [0.0863, 0.0863, 0.0863,  ..., 0.0863, 0.0863, 0.0863],\n          [0.0863, 0.0863, 0.0863,  ..., 0.0863, 0.0863, 0.0863],\n          [0.0863, 0.0863, 0.0863,  ..., 0.0863, 0.0863, 0.0863]]]],\n       device='cuda:0')\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "ValueError",
     "evalue": "In training mode, targets should be passed",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-84-e32f9d25d942>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mboxes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'boxes'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFloatTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torchvision/models/detection/generalized_rcnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, images, targets)\u001b[0m\n\u001b[1;32m     58\u001b[0m         \"\"\"\n\u001b[1;32m     59\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtargets\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"In training mode, targets should be passed\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0mtargets\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: In training mode, targets should be passed"
     ]
    }
   ],
   "source": [
    "while(True):\n",
    "    ret, input = cap.read()\n",
    "    image = input.copy()\n",
    "    input = preprocess(input).float()\n",
    "    input = input.unsqueeze_(0)\n",
    "    input = input.type(torch.cuda.FloatTensor)\n",
    "\n",
    "    print(input)\n",
    "\n",
    "    result = model(input)\n",
    "\n",
    "    boxes = result[0]['boxes'].type(torch.cuda.FloatTensor)\n",
    "    scores = result[0]['scores'].type(torch.cuda.FloatTensor)\n",
    "    labels = result[0]['labels'].type(torch.cuda.FloatTensor)\n",
    "\n",
    "    mask = nms(boxes,scores,0.3)\n",
    "    boxes = boxes[mask]\n",
    "    scores = scores[mask]\n",
    "    labels = labels[mask]\n",
    "\n",
    "    boxes = boxes.data.cpu().numpy().astype(np.int32)\n",
    "    scores = scores.data.cpu().numpy()\n",
    "    labels = labels.data.cpu().numpy()\n",
    "\n",
    "    mask = scores >= 0.5\n",
    "    boxes = boxes[mask]\n",
    "    scores = scores[mask]\n",
    "    labels = labels[mask]\n",
    "\n",
    "    colors = {1:(0,255,0), 2:(255,255,0), 3:(255,0,0)}\n",
    "\n",
    "    for box,label in zip(boxes,labels):\n",
    "        image = cv2.rectangle(image,\n",
    "                          (box[0], box[1]),\n",
    "                          (box[2], box[3]),\n",
    "                          (0,0,255), 1)\n",
    "\n",
    "    cv2.imshow(\"image\", image)\n",
    "    \n",
    "    if cv2.waitKey(0):\n",
    "        break\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}